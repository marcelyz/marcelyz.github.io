---
layout: post
title:  "LSM-Tree"
author: "marcelyz"
---

- **LSM-Tree中的读放大、写放大、空间放大**  
答：LSM-Tree能将离散的磁盘随机写请求都转换成内存随机写+顺序写(WAL) + 定期归并(Compaction)的模式，以此提高性能，但也带来了一些问题。  
读放大：LSM-Tree的读操作需要从新到旧(从上到下)一层一层查找，直到找到想要的数据。这个过程可能需要不止一次IO操作。  
空间放大：因为所有的写入都是顺序写(append-only)，不是in-place update，所以过期数据不会马上清理。
写放大：LSM-Tree通过后台Compaction操作来减少读放大(减少SST文件数量)和空间放大(清理过期数据)，但也因此带来了写放大问题，同一份数据会写好多次。  
优化：业界有采用kv存储分离的方式去优化读写空间放大的问题，由于key所占空间通常远小于value的空间，因此lsm-tree的层数会很少，不会有太多读写放大的问题，将value存在额外的一个日志文件中。  

- **LSM-Tree和B+ Tree的比较**  
答：B+树的优势：  
1. 单一节点存储更多的元素，使得查询的IO次数更少。  
2. 所有查询都要查到叶子节点，查询性能稳定。  
3. 所有叶子节点形成有序链表，便于范围查询。  
4. 比较适用于事务性分析的场景。  
LSM-Tree的优势：  
1. LSM顺序写，批量特性，写入性能更好。比较适用于写多于读且时效性比较强(最近数据最常访问)的场景。  
2. LSM读性能也不差，通过内存命中率，以及布隆过滤器、compaction等操作，也有不俗的读性能。写入不占用磁盘的IO，读取就能获取更长时间的磁盘IO使用权，也能提高读取效率。

- **为什么选rocksdb作为flink的状态后端，为啥能支持增量**  
答：RocksDB 是由 Facebook 基于 LevelDB 开发的一款提供键值存储与读写功能的 LSM-tree 架构引擎。用户写入的键值对会先写入磁盘上的 WAL (Write Ahead Log)，然后再写入内存中的跳表（SkipList，这部分结构又被称作 MemTable）。LSM-tree 引擎由于将用户的随机修改（插入）转化为了对 WAL 文件的顺序写，因此具有比 B 树类存储引擎更高的写吞吐。而键值的特性和flink的keyed state非常相似，rocksdb本身又比较轻量，非常方便嵌入到其系统之中，因此让其作为状态后端的选择。  
但是，rocksdb本身存在读写空间放大的问题，而且如果为了读性能把memtable设置的足够大时，WAL也会很大，这样宕机的话需要很长时间才能恢复。  
RocksDB的数据存储是一个分层的结构，当memtable写满之后，会将其写入磁盘，成为一个不可变且有序的sstable。Flink会跟踪RocksDB自上一个checkpoint以来创建和删除了哪些sstable文件，将所有新的sstable复制到持久化存储(如HDFS)，之前已经存在的sstable不会复制，而是引用它们，这就是Flink增量checkpoint能够切断历史数据的原因。  

- **ssd和lsm-tree**  
答：ssd的特性主要集中在三个方面：1.随机读写和顺序读写的性能差别没有hdd那么大；2.ssd拥有了并发随机读能力，速度很快；3.ssd有数据擦除次数限制，过量的写入会导致ssd报废。  
毫无疑问，ssd具有更加优异的随机读写性能，lsm-tree使用ssd性能提升是非常明显的。但是由于ssd特有的写入擦除循环和昂贵的垃圾回收，我们一般认为对于ssd来说，过量的随机写依旧是有害的，应当尽力避免随机写，多使用顺序写，所以在lsm-tree中使用ssd还是有意义的。  

- **clickhouse的索引**  
答：Clickhouse索引的特点为: 排序索引 + 稀疏索引 + 列式存储, 因此相应的Clickhouse最合适的场景就是基于排序字段的范围过滤后的聚合查询。  
因为排序索引, 所有基于排序字段的查询会明显优于MR类型计算, 否则Hive/Spark这类动态资源的更优。  
由于稀疏索引, 点查询的效率可能没有KV型数据库高, 因此适合相对大范围的过滤条件。  
因为列式存储, 数据压缩率高, 对应做聚合查询效率也会更高。  

- **clickhouse的优缺点**  
答：clickhouse的定位还是分析型数据库，不是严格的关系型数据库，在批量读写以及一些聚合分析上有优势。  
优点：  
    * 列存，向量化处理(批量处理)，高效利用cpu，并行处理优化
    * 数据压缩空间大，减少IO  
    * 批量读写性能高，聚合查询效率高  
缺点：
    * 不支持事务，不支持真正的删除/更新  
    * 不支持高并发，因为CK采用了并行处理机制，即使一个查询，也会用服务器一半的CPU去执行
    * join支持不好
    * 单条insert性能差，因为CK底层会不断地做数据异步合并  
<br>

- **SSTable是啥**  
答：全称是Sorted String Table，排序的字符串表，是一个键有序的，存储字符串形式键值对的文件，同时做了优化来实现顺序读/写操作的高吞吐量。  
Google Bigtable论文中对SSTable的介绍：SSTable提供一个可持久化、有序的、不可变的从键到值的映射关系，其中键和值都是任意字节长度的字符串。SSTable提供了以下操作：按照某个键来查询关联值，可以指定键的范围，来遍历其中所有的键值对。每个SSTable内部由一系列block组成(通常大小为64kb，可配置)。使用存储在SSTable中的index来定位block；当SSTable打开时，index会被加载到内存里，一次磁盘寻址就可以完成查询；首先通过二分查找找到内存索引对应的block，然后在磁盘上读取这块内容。  

- **SSTable(sorted strings table)文件格式**  
Footer：固定48字节，元信息的元信息，指IndexBlock和MetaIndexBlock在文件中的偏移量，位于sstable文件的尾部。  
IndexBlock：占用一个block空间，记录DataBlock的元信息。  
MetaIndexBlock：占用一个block空间，各个元信息的Block，包括Filter、Properties、Compression dictionary、Range deletion tombstone等。  
MetaBlock：可能占用多个block空间，存储布隆过滤器的二进制数据及其他元数据信息。  
DataBlock：可能占用多个block空间，存储实际的数据即键值对内容。  
<br>

- **Rocksdb读流程**  
(1)从内存读:MemTable->Immutable MemTable；  
(2)从持久化设备读：首先通过table cache获取到文件的元数据如布隆过滤器以及数据块索引, 然后读取block cache, block cache中缓存了SST的数据块,如果命中那就直接读取成功,否则便需要从SST文件中读取数据块并插入到block cache。  

- **Rocksdb写流程**  
(1)单线程写WAL日志：保存当前rocksdb的memtable中的文件信息，保证数据在恢复时的可靠性；  
(2)多线程写MemTable：写满之后被标记为read-only，成为immutable-memtable；  
(3)flush到L0层：当immutable-memtable的数量超过阈值时，会把数据flush到磁盘。flush过程以column family(逻辑分区)为单位，一个column family包含一个或多个immutable-memtable，flush会一次性把所有文件合并后写入磁盘的L0层sstable文件中；  
(4)定期compaction：合并的条件是文件个数和文件大小。  
