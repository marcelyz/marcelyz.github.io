---
layout: post
title:  "LSM-Tree"
author: "marcelyz"
---

- 问：LSM-Tree中的读放大、写放大、空间放大  
答：LSM-Tree能将离散的随机写请求都转换成批量的顺序写请求(WAL + Compaction)，以此提高性能，但也带来了一些问题。  
读放大：LSM-Tree的读操作需要从新到旧(从上到下)一层一层查找，直到找到想要的数据。这个过程可能需要不止一次IO操作。  
空间放大：因为所有的写入都是顺序写(append-only)，不是in-place update，所以过期数据不会马上清理。
写放大：LSM-Tree通过后台Compaction操作来减少读放大(减少SST文件数量)和空间放大(清理过期数据)，但也因此带来了写放大问题，同一份数据会写好多次。

- 问：LSM-Tree和B+ Tree的比较  
答：B+树的优势：  
1. 单一节点存储更多的元素，使得查询的IO次数更少。  
2. 所有查询都要查到叶子节点，查询性能稳定。  
3. 所有叶子节点形成有序链表，便于范围查询。  
LSM-Tree的优势：  
1. LSM顺序写，批量特性，写入性能更好。  
2. LSM读性能也不差，通过内存命中率，以及布隆过滤器、compaction等操作，也有不俗的读性能。写入不占用磁盘的IO，读取就能获取更长时间的磁盘IO使用权，也能提高读取效率。

- 问：为什么选rocksdb作为flink的状态后端，为啥能支持增量  
答：RocksDB 是由 Facebook 基于 LevelDB 开发的一款提供键值存储与读写功能的 LSM-tree 架构引擎。用户写入的键值对会先写入磁盘上的 WAL (Write Ahead Log)，然后再写入内存中的跳表（SkipList，这部分结构又被称作 MemTable）。LSM-tree 引擎由于将用户的随机修改（插入）转化为了对 WAL 文件的顺序写，因此具有比 B 树类存储引擎更高的写吞吐。而键值的特性和flink的keyed state非常相似，rocksdb本身又比较轻量，非常方便嵌入到其系统之中，因此让其作为状态后端的选择。  
RocksDB的数据存储是一个分层的结构，当memtable写满之后，会将其写入磁盘，成为一个不可变且有序的sstable。Flink会跟踪RocksDB自上一个checkpoint以来创建和删除了哪些sstable文件，将所有新的sstable复制到持久化存储(如HDFS)，之前已经存在的sstable不会复制，而是引用它们，这就是Flink增量checkpoint能够切断历史数据的原因。