---
layout: post
title:  "spark简介"
author: "marcelyz"
---


## 一、算子相关
- **reduceByKey和groupByKey的区别**  
答：groupByKey先分组后计算，代价较高；reduceByKey先部分计算，再分组聚合，性能较好  

- **map、flatMap、mapPartitions、mapValues、mapKeys、mapGroups、flatMapGroups的区别**  
答：flatMap会对map结果flatten；mapPartitions对分区进行map，map的次数较少；mapValues对value进行操作，mapKeys对key进行操作；mapGroups对group进行操作，通常与groupByKey一起使用；flatMapGroups有flatten的作用。  

- **mapPartitions、foreachPartition的区别**  
返回值区别：map、mapPartitions属于Transformation操作，延迟触发，返回值一般是rdd/df；foreach、foreachPartition是Action操作，立即触发，返回值一般是数值、object、unit等。  
使用区别：map系列一般用于链式的转化，对rdd/df做一些惰性变换；foreach系列一般都是在程序末尾比如说要落地数据到存储系统中如mysql，hdfs中使用。  

- **怎么区分Transformation和Action操作**  
答：看返回值，Transformation操作一般返回rdd或df类型(如大部分结尾带key的算子)，而action操作一般返回数值、数据结构(如map)或者不返回任何值(如写磁盘)。  

- **groupBy、groupByKey的区别**  
答：groupBy根据特定列进行分组，多用在ds和df中；groupByKey根据特定key进行分组，多用在rdd中。

- **join、joinWith的区别**  
答：都是用来join数据的，区别在于返回结果的schema不同，joinWith返回一个嵌套的tuple，保留了原有df的数据类型。

- **aggregate、treeAggregate的区别**  
答：aggregate是在每个分区计算完成后，把所有的数据拉倒driver端，进行统一的遍历合并，这样如果数据量很大，很容易出现单点数据聚合瓶颈，在driver端可能会OOM；treeAggregate增加了分层树形聚合，启动reduce task对各个map task输出的结果进行两两聚合，然后在聚合后的结果上再进行两两聚合，直至剩余很少的中间聚合结果时，再在driver端进行聚合累加，该算子多用在机器学习的迭代计算梯度中。虽然可以减轻driver端的压力，但是聚合操作的延时会有所增加。

- **DStream.transform、DStream.transformWith的区别**  
答：都是指在DStream上应用transformFunc方法，返回一个新DStream；区别在于transformWith的入参可以包含其他DStream。

- **HashPartitioner和RangePartitioner的区别，如何自定义partitioner，repartition、coalesce的区别**  
答：在分区时使用，指定分区的方式。  
HashPartitioner指的是使用Java的Object.hashCode来分区；  
RangePartitioner指的是通过"水塘抽样"算法对Key进行排序，它能保证各个Partition之间的Key是有序的，并且各个Partition之间数据量差不多，但是不保证单个Partition内Key的有序性。  
repartition一般在分区数变多的时候使用，会经过shuffle；coalesce一般在分区数变少的时候使用，默认不经过shuffle，但是有可能有数据倾斜，可以手动指定shuffle。  

- **spark.sql.shuffle.partitions和spark.default.parallelism的区别，分别在什么时候起作用**  
答：spark.default.parallelism这个参数只是针对rdd的shuffle操作才生效，spark.sql.shuffle.partitions是对sparkSQL进行shuffle操作(比如窗口函数)的时候生效；这两个参数一般是一起设置成核数的2-3倍


## 二、概念相关
- **RDD、Dataframe、DataSet的区别是什么**  
答：RDD：不可变的弹性分布式数据集、非结构化、较底层，弹性体现在数据分片repartion；自动进行内存和磁盘的切换；基于血缘关系的高效容错等  
DataFrame：带有schema信息，支持定制的视图和结构；以及方便易用的结构化api，使用Catalyst来生成查询计划，性能上有较大提升。  
DataSet：强类型，在编译时进行类型检测  

- **master和driver以及worker和executor的区别；spark任务的提交过程，driver端如何创建dag，dag scheduler的作用，如何分发任务到executor**  
答：master和worker都是具体的节点；driver进程可以在任意节点(client、master、worker)，负责构建sparkContext对象、作业的解析，向集群管理者申请资源；executor进程持有一个线程池，每个线程执行一个task；master守护进程主要负责与worker的交互。
driver进程解析我们编写的spark应用代码，通过transformation算子来转换rdd，通过action算子拆分成多个job，每个job根据依赖关系划分为多个stage，形成dag，每个stage创建一批tasks，然后将这些tasks分配到各个executor中执行。

- **宽窄依赖**  
答：宽窄依赖表示rdd之间的依赖关系。宽依赖指的是多个子 RDD 的 partition 会依赖同一个 parent RDD的 partition（多子一亲，超生）  
窄依赖指的是每一个 parent RDD 的 partition 最多被子 RDD 的一个 partition 使用（一子一亲，只有一个孩子）  

- **Spark shuffle write 和 shuffle read**  
答：shuffle机制是指运行在不同stage、不同节点的task间进行数据传递的过程，分为shuffle write和shuffle read两个阶段，前者主要解决上游stage输出数据的分区问题，计算顺序大致为数据聚合->排序->分区输出，后者主要解决下游stage从上游stage获取数据、重新组织、并为后续操作提供数据的问题，计算顺序大致为数据获取->聚合->排序。在shuffle过程中，聚合操作基于类似HashMap数据结构完成，排序操作基于了类似Array数据结构完成，内存空间不够则spill到磁盘，最后将磁盘和内存中的数据进行聚合、排序，得到最终结果。

- **Spark hash base shuffle 和 sort base shuffle**  
答：hash和sort是shuffle实现的两种方式，spark初始版本是通过Hash实现的，但是在shuffle时会产生大量文件，这严重制约了spark的性能及扩展能力。因此在后面的版本迭代中产生了sort shuffle方式，它不会为每个task生成一个单独的文件，而是对数据进行排序，然后写到一个数据文件中，同时生成一个索引文件，各个task通过索引文件获取相关的数据。sort模式有三种运行机制，普通运行机制、bypass运行机制、Tungsten sort运行机制，分别对应不同场景下的优化。

- **存储格式：orc、parquet、avro区别**  
答：avro是一种基于行的存储格式，编写操作方便。parquet和orc是基于列的存储格式，parquet更能存储嵌套数据；而orc能支持ACID属性，不过orc跟hive接触更紧密，和Impala、presto等查询引擎适配并不好。一般情况下建议使用parquet。
列存的格式，可以带来更高的压缩比，以及更小的io操作。但是列存储不适合oltp场景，因为有大量随机的一行一行的读取操作，如果是列存储，需要循环从各个列中取的相应字段，效率不高。

- **parquet存储模型**  
答：主要由行组(Row Group)、列块(Column Chunk)、页(Page)组成，以及header和footer信息，分别存储文件的校验码和schema等信息；行组是parquet文件在水平方向上的数据划分，默认大小与HDFS Block块大小对齐，保证一个行组会被一个Mapper处理。行组中每一列的数据保存在一个列块中，一个列块具有相同的数据类型，不同列可以使用不同的压缩方式。每一个列块可以包含多个页，页是最小的编码单位。

- **压缩算法：lzo、snappy、gzip区别**  
答：一般来说说压缩率和压缩速度成反比，这三者压缩率gzip>lzo>snappy，不过gzip是cpu密集型的，对cpu的消耗对比其他算法要高很多。压缩和解压速度snapy>lzo>gzip。

- **hive内部表外部表区别**  
答：外部表会将数据存储在别处，当删除表的时候，只会删除表的元数据，不会删除真正存储的数据。一般一些最原始的服务日志会使用外部表存储，安全。

- **spark streaming/flink流计算中的Exactly-once语义保证**  
答：每一条记录只会被正确处理(读取、计算、存储)一次，或者多次处理跟一次处理的效果相同，即使服务器或网络发生故障再重新处理时也能保证没有偏差，即Exactly-once语义。这不仅需要实时计算框架本身的支持，还对上游的消息系统、下游的数据存储有所要求。  
消息系统：hdfs天然支持Exactly-once语义，kafka设置在处理流程结束时再提交位点也可以支持；  
计算流程：spark中的dag具有容错性、计算不变性等，也能保证该语义；  
存储：可通过幂等写入和事务写入支持Exactly-once语义。  
flink：通过checkpoint机制可以保证精准一次；端到端的精准一次需要source和sink本身系统支持，才能保证，一般来说，如果组件支持幂等，或者事务回滚机制，可以通过两阶段提交达到端到端的精准一次语义，比如kafka，mysql，redis等。

- **cahce和checkpoint的区别**  
答：目的不同，cache是为了增加作业的效率，避免冗余计算。checkpoint是防止数据依赖关系比较复杂的中间数据丢失，使job失败后能够快速恢复。虽然都可以持久化到磁盘，不过一旦driver program执行结束，被cache到磁盘上的RDD就会被清空；而被checkpoint的RDD会一直存在，可以被下一次driver程序使用；但是cache后的rdd能保存dag的血缘关系。

- **spark错误容忍机制**  
答：核心方法有两种，一是通过dag血缘关系机制来重新执行计算任务。当job抛出异常不能继续执行时，重新启动计算任务，再次执行，容错保证。二是通过checkpoint机制，对一些重要的输入/输出、中间数据进行持久化。这可以在一定程度上解决数据丢失问题，而且能够提高任务重新计算时的效率。

- **spark的几种部署模式**  
    1. 本地模式：在本地起多个线程来运行，有三类，local，local[k]，local[*]  
    2. standalone：自带完整的服务、资源管理和任务监控，是其他模式的基础  
    3. yarn模式：分布式部署集群、资源管理和任务监控都交给yarn，但是目前只支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产环境，driver运行在集群子节点，具有容错功能；client模式适合调试，driver运行在client节点  
    4. mesos模式：官方更推荐这种，有两种模式，粗粒度和细粒度；粗粒度指的是任务运行前预先把资源都申请好，程序结束后释放，这样会造成大量的资源浪费。细粒度类似现在的云计算模式，思想是按需分配。  
<br>

- **Spark为什么比mapreduce快**  
    1. 内存和磁盘使用方面: Spark vs MapReduce不等于内存vs磁盘，Spark和MapReduce的计算都发生在内存中，区别在于：  
        - MapReduce需要将每次计算的结果写入磁盘，然后再从磁盘读取数据，从而导致了频繁的磁盘IO。  
        - Spark通常不需要将计算的结果写入磁盘，可以在内存中进行迭代计算。这得益于Spark的RDD和DAG（有向无环图），其中DAG记录了job的stage以及在job执行过程中父RDD和子RDD之间的依赖关系。中间结果能够以RDD的形式存放在内存中，极大减少了磁盘IO。  
    2. Shuffle上的不同: Spark和MapReduce在计算过程中通常都不可避免的会进行Shuffle，Shuffle都会落盘，但：  
        - MapReduce在Shuffle时需要花费大量时间进行排序，排序在MapReduce的Shuffle中似乎是不可避免的；  
        - Spark在Shuffle时则只有部分场景才需要排序，支持基于Hash的分布式聚合，更加省时；  
    3. 任务级别并行度上的不同  
        - MapReduce采用了多进程模型，而Spark采用了多线程模型。多进程模型的好处是便于细粒度控制每个任务占用的资源，但每次任务的启动都会消耗一定的启动时间。就是说MapReduce的Map Task和Reduce Task是进程级别的，都是 jvm 进程，每次启动都需要重新申请资源，消耗了不必要的时间。  
        - Spark Task则是基于线程模型的，通过复用线程池中的线程来减少启动、关闭task所需要的开销。（多线程模型也有缺点，由于同节点上所有任务运行在一个进程中，因此，会出现严重的资源争用，难以细粒度控制每个任务占用资源）  
<br>

## 三、原理相关
- **spark join是怎么实现的**  
答：通过Spark hash base shuffle 和 sort base shuffle实现的，broadcast join也叫map端join，会在driver端把小表的数据都广播到executor端。

- **spark物理执行计划生成步骤**  
答：spark具体采用3个步骤来生成物理执行计划：首先根据action()操作顺序将应用划分为多个作业job，然后根据每个job的逻辑处理流程中的ShuffleDependency依赖关系，将job划分为多个执行阶段stage。最后在每个stage中，根据最后生成的RDD的分区个数生成多个计算任务task。

- **数据倾斜出现的原因(groupBy倾斜、join倾斜)和对应处理方案**  
答：数据倾斜的现象是大部分task执行很快，个别task执行执行极慢，导致程序整体耗时增加。  
原因是在进行shuffle的时候(如distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等)，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行groupBy聚合或join等操作，如果某个key对应的数据量特别大，task分配不均匀，就会导致数据倾斜。  
处理方案的唯一原则是让各个task的数据量均匀；首先从业务层面上去优化，比如对null值进行处理，有时候null值能造成严重的笛卡尔积，数据量暴增；另一方面看能否优化业务逻辑，通过filter等减少shuffle数据量，过滤倾斜key，filter操作完之后记得重分区，不然也会导致不均匀；  
如果实在不能减少，先尽量排查出有倾斜的具体key，然后对这些key进行加盐操作,通过在join/group的keys中新增一列来使shuffle后的数据均匀；如对小表增加一列skew_key，使用explode(lit((0 to n).toArray))扩容n倍；对大表也增加一列skew_key，使用[monotonically_increasing_id() % n]来增加n以内的随机数，两表的n必须相等，这样才能不丢数；n越大，从概率上说倾斜的数据分配的就越均匀，不过占用的内存也越大。  
最后还有一些优化方法，比如只选取必要字段去join，这样shuffle数据占用的内存会大幅度减小，join完之后再反join原始数据，获得其他字段。以及通过业务逻辑转变join key，比如将device join转变成clickid join，缓解数据倾斜。
refs: https://coxautomotivedatasolutions.github.io/datadriven/spark/data%20skew/joins/data-skew-2/

- **spark的内存模型，executor的内存划分**  
答：spark内存分为堆内内存(on-heap)和堆外内存(off-heap)，其中堆内内存基于jvm内存模型；而堆外内存则是通过调用底层jdk unsafe api。两种内存类型统一由spark内存管理MemoryManager接口实现。  
堆内内存：包含storage、execution、other三部分。storage用于缓存RDD数据和broadcast广播变量的内存使用；execution提供shuffle过程中的内存使用；other提供spark内部对象、用户自定义对象的内存空间。  
堆外内存：此模式不在jvm中申请内存，而是直接使用操作系统内存，减少了jvm中内存空间切换的开销，降低了GC回收占用的消耗，实现对内存的精确管控。这部分内存仅包含storage和execution，且被所有task线程共享。  
spark 1.6之前，spark采用的是静态管理模式，内存分配占比是不变的。在1.6之后，引入了统一管理模式，支持storage和execution内存动态占用，便于调优和维护，提升内存的合理利用率。  

- **spark broadcast实现**  
答：广播变量大致分为以下3个流程:  
1.Driver端创建广播对象，将对象分块，每个块作为一个block存进Driver端的BlockManager；  
2.Broadcast对象在Driver端进行序列化，在Executor端进行反序列化。  
3.每个Executor会试图获取所有的块，来组装成一个被Broadcast的变量。"获取块"的方法是首先从Executor自身的BlockManager中获取，如果自己的BlockManager中没有这个块，就从别的BlockManager中获取。  
这样子使得本来每个Task都需要拉取的变量，变成了每个Executor拉取一个变量即可，减少了文件传输和GC开销，从而加速了程序运行速度；另外使用BT协议进行广播数据，程序刚开始Driver是获取这些块的唯一的源，随着各个Executor的BlockManager从Driver端获取了不同的块，就可以从不同地方获取，缓解driver单点压力。  

- **spark 3.0 AQE(Adaptive Query Execution，自适应查询执行)**  
答：AQE是spark sql的一种动态优化机制，在运行含有shuffle的spark sql任务时，每当shuffle read阶段执行完毕，AQE都会结合这个阶段的统计信息，基于既定的规则动态调整、修正尚未执行的逻辑计划和物理计划，来完成对原始查询语句的运行时优化。类似于CBO的优化策略，不同的是CBO是基于表的统计信息，AQE是基于shuffle中间落盘的临时文件的统计信息。AQE的三大特性：自动的分区合并、动态选择join策略，自动处理数据倾斜。  
自动的分区合并：在shuffle过后，Reduce task数据分布参差不齐，AQE将自动合并过小的数据分区。  
动态选择join策略：如果某张表在过滤之后，尺寸小于广播变量阈值，那么join策略会从shuffle sort merge join优化为broadcast hash join。  
自动处理数据倾斜：自动拆分Reduce阶段过大的数据分区，将其打散，降低单个task的工作负载。  

- **spark GC**  
答：spark GC调优的目标是，确保生命周期比较长的RDD保存在老年代，新生代有足够的空间保存生命周期比较短的对象，这样有助于避免触发full GC去收集运行期间产生的临时变量。  
收集gc信息：spark-submit的时候，添加PrintGCDetails和PrintGCTimeStamps等选项，在worker节点打印gc的详细信息。  
修改不同内存区域比例：在gc统计信息中，如果有太多的full gc，说明老年代满了，可以适当增加老年代的内存比例，或者减少用于缓存的内存；如果有太多的minor gc，可以适当增加Eden区的内存占比。  
使用G1垃圾回收器：G1兼顾了吞吐量和停顿时间，可以避免内存碎片，尤其在堆非常大的时候，G1优势很明显。  

- **spark sql的执行过程**  
答：
